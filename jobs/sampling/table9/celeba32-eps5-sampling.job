#!/bin/bash

#SBATCH --partition=gpu_mig
#SBATCH --gpus=1
#SBATCH --job-name=t9_c32_e5
#SBATCH --ntasks=1
#SBATCH --cpus-per-task=8
#SBATCH --time=8:00:00
#SBATCH --output=slurm_output_%A.out

module purge
module load 2024
module load Anaconda3/2024.06-1

source activate ldm
cd ~/fact/dp_lora
mkdir -p output/table9

python ./sampling/unconditional_sampling.py \
    --yaml ./reproducibility_experiments/finetuning/celeba32-dp-eps5.yaml \
    --ckpt <PATH_TO_CELEBA32_EPS5_CKPT> \
    -o output/table9/celeba32_eps5.pt \
    --num_samples 60000 \
    --batch_size 300
